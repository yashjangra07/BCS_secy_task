{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e315cc5-9749-46e0-8be1-4d106fffa9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Step 1: Import Required Libraries\n",
    "import re\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, confusion_matrix, roc_curve, auc, \n",
    "                            RocCurveDisplay)\n",
    "\n",
    "#%% Step 2: Data Preparation\n",
    "# Load datasets\n",
    "fake_df = pd.read_csv(\"Fake[1].csv\")\n",
    "true_df = pd.read_csv(\"True[1].csv\")\n",
    "\n",
    "# Add labels and combine\n",
    "fake_df['label'] = 0\n",
    "true_df['label'] = 1\n",
    "combined_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "\n",
    "# Combine title and text\n",
    "combined_df['full_text'] = combined_df['title'] + \" \" + combined_df['text']\n",
    "\n",
    "#%% Step 3: Custom Tokenizer Implementation\n",
    "class VeritasTokenizer:\n",
    "    def __init__(self):\n",
    "        self.emoticons = r\"\"\"\n",
    "            (?:\n",
    "                [<>]?\n",
    "                [:;=8]                     # eyes\n",
    "                [\\-o\\*\\']?                 # optional nose\n",
    "                [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "                |\n",
    "                [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "                [:;=8]                     # eyes\n",
    "                [\\-o\\*\\']?                 # optional nose\n",
    "                [<>]?\n",
    "            )\"\"\"\n",
    "        self.contractions = re.compile(r\"(\\b\\w+\\b)(n't|'ll|'ve|'re|'d|'s|'m)\\b\")\n",
    "        self.repeats = re.compile(r\"(\\w*)(\\w)\\2{2,}(\\w*)\")\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        # Handle emoticons\n",
    "        text = re.sub(self.emoticons, lambda m: \" \" + m.group() + \" \", text, flags=re.VERBOSE)\n",
    "        \n",
    "        # Handle contractions\n",
    "        text = self.contractions.sub(r\"\\1 \\2\", text)\n",
    "#%% Step 1: Import Required Libraries\n",
    "import re\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, confusion_matrix, roc_curve, auc, \n",
    "                            RocCurveDisplay)\n",
    "\n",
    "#%% Step 2: Data Preparation\n",
    "# Load datasets\n",
    "fake_df = pd.read_csv(\"Fake[1].csv\")\n",
    "true_df = pd.read_csv(\"True[1].csv\")\n",
    "\n",
    "# Add labels and combine\n",
    "fake_df['label'] = 0\n",
    "true_df['label'] = 1\n",
    "combined_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "\n",
    "# Combine title and text\n",
    "combined_df['full_text'] = combined_df['title'] + \" \" + combined_df['text']\n",
    "\n",
    "#%% Step 3: Custom Tokenizer Implementation\n",
    "class VeritasTokenizer:\n",
    "    def __init__(self):\n",
    "        self.emoticons = r\"\"\"\n",
    "            (?:\n",
    "                [<>]?\n",
    "                [:;=8]                     # eyes\n",
    "                [\\-o\\*\\']?                 # optional nose\n",
    "                [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "                |\n",
    "                [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "                [:;=8]                     # eyes\n",
    "                [\\-o\\*\\']?                 # optional nose\n",
    "                [<>]?\n",
    "            )\"\"\"\n",
    "        self.contractions = re.compile(r\"(\\b\\w+\\b)(n't|'ll|'ve|'re|'d|'s|'m)\\b\")\n",
    "        self.repeats = re.compile(r\"(\\w*)(\\w)\\2{2,}(\\w*)\")\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        # Handle emoticons\n",
    "        text = re.sub(self.emoticons, lambda m: \" \" + m.group() + \" \", text, flags=re.VERBOSE)\n",
    "        \n",
    "        # Handle contractions\n",
    "        text = self.contractions.sub(r\"\\1 \\2\", text)\n",
    "        \n",
    "        # Handle repeated characters\n",
    "        text = self.repeats.sub(lambda m: f\"{m.group(1)}{m.group(2)} <REPEAT:{len(m.group(0))-len(m.group(1)+m.group(3))}> {m.group(3)}\", text)\n",
    "        \n",
    "        # Final tokenization\n",
    "        tokens = re.findall(r\"\"\"\n",
    "            \\b\\w+['â€™]?\\w*\\b|          # Words with apostrophes\n",
    "            {emoticons}|               # Emoticons\n",
    "            <REPEAT:\\d+>|             # Repeat tokens\n",
    "            [.,!?;:()\\\"'`]            # Punctuation\n",
    "        \"\"\".format(emoticons=self.emoticons), text, flags=re.VERBOSE)\n",
    "        \n",
    "        return [token.lower() for token in tokens if token.strip()]\n",
    "\n",
    "#%% Step 4: Rule-Based POS Tagger\n",
    "class VeritasPOSTagger:\n",
    "    def __init__(self):\n",
    "        self.rules = {\n",
    "            'VERB': [r'ing$', r'ed$', r's$'],\n",
    "            'ADJ': [r'ous$', r'ive$', r'al$', r'ic$'],\n",
    "            'ADV': [r'ly$']\n",
    "        }\n",
    "        \n",
    "    def tag(self, tokens):\n",
    "        tagged = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            pos = 'NOUN'  # Default\n",
    "            for pos_type, patterns in self.rules.items():\n",
    "                for pattern in patterns:\n",
    "                    if re.search(pattern, token):\n",
    "                        pos = pos_type\n",
    "                        break\n",
    "            # Special case for verbs after 'to'\n",
    "            if i > 0 and tokens[i-1] == 'to' and pos == 'NOUN':\n",
    "                pos = 'VERB'\n",
    "            tagged.append((token, pos))\n",
    "        return tagged\n",
    "\n",
    "#%% Step 5: Custom Lemmatizer\n",
    "class VeritasLemmatizer:\n",
    "    def __init__(self):\n",
    "        self.suffix_map = {\n",
    "            'VERB': [('ing', ''), ('ed', 'e'), ('s', '')],\n",
    "            'NOUN': [('s', ''), ('ies', 'y'), ('es', '')],\n",
    "            'ADJ': [('er', ''), ('est', '')]\n",
    "        }\n",
    "        \n",
    "    def lemmatize(self, token, pos):\n",
    "        token = token.lower()\n",
    "        for suffix, replacement in self.suffix_map.get(pos, []):\n",
    "            if token.endswith(suffix):\n",
    "                return token[:-len(suffix)] + replacement\n",
    "        return token\n",
    "\n",
    "#%% Step 6: Full Text Processing Pipeline\n",
    "tokenizer = VeritasTokenizer()\n",
    "pos_tagger = VeritasPOSTagger()\n",
    "lemmatizer = VeritasLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = tokenizer.tokenize(str(text))\n",
    "    tagged = pos_tagger.tag(tokens)\n",
    "    return [lemmatizer.lemmatize(token, pos) for token, pos in tagged]\n",
    "\n",
    "# Process sample data\n",
    "combined_df['processed'] = combined_df['full_text'].apply(process_text)\n",
    "\n",
    "#%% Step 7: Feature Extraction\n",
    "# Convert processed text to strings for vectorization\n",
    "combined_df['processed_str'] = combined_df['processed'].apply(' '.join)\n",
    "\n",
    "# Create TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X = tfidf.fit_transform(combined_df['processed_str'])\n",
    "y = combined_df['label']\n",
    "\n",
    "#%% Step 8: Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#%% Step 9: Model Training\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# SVM (using SGD for efficiency)\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, \n",
    "                   max_iter=1000, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "#%% Step 10: Evaluation\n",
    "def evaluate_model(model, name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    probas = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    if probas is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, probas)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, \n",
    "                       estimator_name=name).plot()\n",
    "        plt.title(f'{name} ROC Curve (AUC = {roc_auc:.4f})')\n",
    "        plt.show()\n",
    "\n",
    "evaluate_model(nb, \"Naive Bayes\")\n",
    "evaluate_model(svm, \"SVM\")\n",
    "\n",
    "#%% Step 11: Visualizations\n",
    "# Word Clouds\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Fake news word cloud\n",
    "fake_text = ' '.join(combined_df[combined_df['label'] == 0]['processed_str'])\n",
    "generate_wordcloud(fake_text, \"Fake News Word Cloud\")\n",
    "\n",
    "# Real news word cloud\n",
    "true_text = ' '.join(combined_df[combined_df['label'] == 1]['processed_str'])\n",
    "generate_wordcloud(true_text, \"Real News Word Cloud\")\n",
    "\n",
    "# Repeat token analysis\n",
    "repeat_counts = Counter()\n",
    "for tokens in combined_df['processed']:\n",
    "    repeat_counts.update([token for token in tokens if '<REPEAT' in token])\n",
    "    \n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=list(repeat_counts.keys())[:10], \n",
    "            y=list(repeat_counts.values())[:10])\n",
    "plt.title(\"Top 10 Repeated Character Patterns\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "        \n",
    "\n",
    "#%% Step 4: Rule-Based POS Tagger\n",
    "class VeritasPOSTagger:\n",
    "    def __init__(self):\n",
    "        self.rules = {\n",
    "            'VERB': [r'ing$', r'ed$', r's$'],\n",
    "            'ADJ': [r'ous$', r'ive$', r'al$', r'ic$'],\n",
    "            'ADV': [r'ly$']\n",
    "        }\n",
    "        \n",
    "    def tag(self, tokens):\n",
    "        tagged = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            pos = 'NOUN'  # Default\n",
    "            for pos_type, patterns in self.rules.items():\n",
    "                for pattern in patterns:\n",
    "                    if re.search(pattern, token):\n",
    "                        pos = pos_type\n",
    "                        break\n",
    "            # Special case for verbs after 'to'\n",
    "            if i > 0 and tokens[i-1] == 'to' and pos == 'NOUN':\n",
    "                pos = 'VERB'\n",
    "            tagged.append((token, pos))\n",
    "        return tagged\n",
    "\n",
    "#%% Step 5: Custom Lemmatizer\n",
    "class VeritasLemmatizer:\n",
    "    def __init__(self):\n",
    "        self.suffix_map = {\n",
    "            'VERB': [('ing', ''), ('ed', 'e'), ('s', '')],\n",
    "            'NOUN': [('s', ''), ('ies', 'y'), ('es', '')],\n",
    "            'ADJ': [('er', ''), ('est', '')]\n",
    "        }\n",
    "        \n",
    "    def lemmatize(self, token, pos):\n",
    "        token = token.lower()\n",
    "        for suffix, replacement in self.suffix_map.get(pos, []):\n",
    "            if token.endswith(suffix):\n",
    "                return token[:-len(suffix)] + replacement\n",
    "        return token\n",
    "\n",
    "#%% Step 6: Full Text Processing Pipeline\n",
    "tokenizer = VeritasTokenizer()\n",
    "pos_tagger = VeritasPOSTagger()\n",
    "lemmatizer = VeritasLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = tokenizer.tokenize(str(text))\n",
    "    tagged = pos_tagger.tag(tokens)\n",
    "    return [lemmatizer.lemmatize(token, pos) for token, pos in tagged]\n",
    "\n",
    "# Process sample data\n",
    "combined_df['processed'] = combined_df['full_text'].apply(process_text)\n",
    "\n",
    "#%% Step 7: Feature Extraction\n",
    "# Convert processed text to strings for vectorization\n",
    "combined_df['processed_str'] = combined_df['processed'].apply(' '.join)\n",
    "\n",
    "# Create TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X = tfidf.fit_transform(combined_df['processed_str'])\n",
    "y = combined_df['label']\n",
    "\n",
    "#%% Step 8: Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#%% Step 9: Model Training\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB(alpha=0.1)\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# SVM (using SGD for efficiency)\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, \n",
    "                   max_iter=1000, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "#%% Step 10: Evaluation\n",
    "def evaluate_model(model, name):\n",
    "    y_pred = model.predict(X_test)\n",
    "    probas = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    if probas is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, probas)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, \n",
    "                       estimator_name=name).plot()\n",
    "        plt.title(f'{name} ROC Curve (AUC = {roc_auc:.4f})')\n",
    "        plt.show()\n",
    "\n",
    "evaluate_model(nb, \"Naive Bayes\")\n",
    "evaluate_model(svm, \"SVM\")\n",
    "\n",
    "#%% Step 11: Visualizations\n",
    "# Word Clouds\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Fake news word cloud\n",
    "fake_text = ' '.join(combined_df[combined_df['label'] == 0]['processed_str'])\n",
    "generate_wordcloud(fake_text, \"Fake News Word Cloud\")\n",
    "\n",
    "# Real news word cloud\n",
    "true_text = ' '.join(combined_df[combined_df['label'] == 1]['processed_str'])\n",
    "generate_wordcloud(true_text, \"Real News Word Cloud\")\n",
    "\n",
    "# Repeat token analysis\n",
    "repeat_counts = Counter()\n",
    "for tokens in combined_df['processed']:\n",
    "    repeat_counts.update([token for token in tokens if '<REPEAT' in token])\n",
    "    \n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=list(repeat_counts.keys())[:10], \n",
    "            y=list(repeat_counts.values())[:10])\n",
    "plt.title(\"Top 10 Repeated Character Patterns\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6690d56-13cb-4b3c-827e-53a872d4cecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
